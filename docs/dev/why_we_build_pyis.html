

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Why We Build PyIS &mdash; Python Inference Script 1.0 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Build from Source" href="build_from_source.html" />
    <link rel="prev" title="Native C++ Library" href="../backends/native_cpp_library.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Python Inference Script
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Model Authoring</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operators/a_list_of_operators.html">Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/a_list_of_tutorials.html">Tutorials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../backends/cpython.html">CPython Backend üêç</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backends/libtorch.html">LibTorch Backend üî•</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backends/onnxruntime.html">ONNXRuntime Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backends/native_cpp_library.html">Native C++ Library</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">For Developers</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Why We Build PyIS</a></li>
<li class="toctree-l1"><a class="reference internal" href="build_from_source.html">Build from Source</a></li>
<li class="toctree-l1"><a class="reference internal" href="house_keeping.html">House Keeping</a></li>
<li class="toctree-l1"><a class="reference internal" href="build_libtorch.html">Build LibTorch for JIT</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Python Inference Script</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Why We Build PyIS</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/dev/why_we_build_pyis.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="why-we-build-pyis">
<h1>Why We Build PyIS<a class="headerlink" href="#why-we-build-pyis" title="Permalink to this headline">¬∂</a></h1>
<div class="section" id="all-in-one-model">
<h2>All-in-one Model<a class="headerlink" href="#all-in-one-model" title="Permalink to this headline">¬∂</a></h2>
<p>When we talk about a model, we most likely mean a neural network graph that transforms input tensors into output tensors, due to the popularity of deep learning methods in recent years. As a result, it is necessary to answer what is a model or what is the boundary between model and application business logics at the very beginning.</p>
<p>In this project, we want the model to cover the end-to-end process from original input to final result. Looking from the model management perspetive, we prefer All-in-one Model to Fragmented Model during authoring and inference.</p>
<div class="figure align-center" id="id1">
<a class="reference internal image-reference" href="../_images/all-in-one-model.png"><img alt="All in One Model" src="../_images/all-in-one-model.png" style="width: 1000.0px; height: 276.0px;" /></a>
<p class="caption"><span class="caption-text">All in One Model</span><a class="headerlink" href="#id1" title="Permalink to this image">¬∂</a></p>
</div>
<p>Firstly, the data processing steps before and/or after the core neural network graph should be part of the model itself. This is in contract to writing data processing logics in the business code of an application with specific programming language, while authoring the neural network part in Python with TensorFlow or PyTorch.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># preprocessing: transform text query to a list of word ids(integer)</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="n">ids</span> <span class="o">=</span> <span class="n">VocabLookUp</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>

<span class="c1"># neural network: output the probabilities over label ids(integer)</span>
<span class="n">prob</span> <span class="o">=</span> <span class="n">NNModel</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>

<span class="c1"># postprocessing: transform label id to text</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">IDToLabel</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
</pre></div>
</div>
<p>Secondly, the glue logics that orchestrate several sub-models should be part of the model itself.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">language</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lang_detect_model</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>

<span class="k">if</span> <span class="n">language</span> <span class="o">==</span> <span class="s1">&#39;en-us&#39;</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enus_model</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">language</span> <span class="o">==</span> <span class="s1">&#39;fr-fr&#39;</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">frfr_model</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">language</span> <span class="o">==</span> <span class="s1">&#39;zh-cn&#39;</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zhcn_model</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enus_model</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, both neural networks, classic statistical and rule-based models should be supported. They are all models. Over the past several years, although neural networks have become the promising choice for almost all machine learning problems, classic models like SVM, LR and GBDT, are still there due to their simplicity, efficiency and legacy reasons.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># classic model</span>
<span class="n">model1</span> <span class="o">=</span> <span class="n">LinearSVMModel</span><span class="p">(</span><span class="n">file</span><span class="o">=</span><span class="s1">&#39;./model/svm.bin&#39;</span><span class="p">)</span>

<span class="c1"># rule-based model</span>
<span class="n">model2</span> <span class="o">=</span> <span class="n">Trie</span><span class="p">(</span><span class="n">file</span><span class="o">=</span><span class="s1">&#39;./model/allow_list.txt&#39;</span><span class="p">)</span>

<span class="c1"># deep model</span>
<span class="n">model3</span> <span class="o">=</span> <span class="n">ONNXModel</span><span class="p">(</span><span class="n">file</span><span class="o">=</span><span class="s1">&#39;./model/lstm.onnx&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="data-driven">
<h2>Data Driven<a class="headerlink" href="#data-driven" title="Permalink to this headline">¬∂</a></h2>
<p>No matter where a model is deployed, server-side or client-side, upgrading models via data or configuration files is superior to binaries. With the later case, we will have to build a new version of the whole application to deploy every time we want a model update.</p>
</div>
<div class="section" id="from-training-to-inference">
<h2>From Training to Inference<a class="headerlink" href="#from-training-to-inference" title="Permalink to this headline">¬∂</a></h2>
<p>How do we reuse code between training and inference?</p>
<p>How do we generate binaries that are needed for inference?</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Model</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Problems: where are the binary files from?</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ngram_extractor</span> <span class="o">=</span> <span class="n">NGramExtractor</span><span class="p">(</span><span class="nb">bin</span><span class="o">=</span><span class="s1">&#39;./ngram.bin&#39;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifer</span> <span class="o">=</span> <span class="n">SVMClassifier</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s1">&#39;./svm_weights.bin&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="n">ngrams</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ngram_extractor</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="n">label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifer</span><span class="p">(</span><span class="n">ngrams</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">label</span>

<span class="n">ngram_extractor</span> <span class="o">=</span> <span class="n">NGramExtractor</span><span class="p">()</span>
<span class="n">ngram_extractor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">queries</span><span class="p">)</span>
<span class="n">ngrams</span> <span class="o">=</span> <span class="n">ngram_extractor</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">queries</span><span class="p">)</span>
<span class="n">ngram_extractor</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="nb">bin</span><span class="o">=</span><span class="s1">&#39;./ngram.bin&#39;</span><span class="p">)</span>

<span class="n">svm</span> <span class="o">=</span> <span class="n">SVMClassifier</span><span class="p">()</span>
<span class="n">svm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">expected_lables</span><span class="p">)</span>
<span class="n">svm</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s1">&#39;./svm_weights.bin&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="build_from_source.html" class="btn btn-neutral float-right" title="Build from Source" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="../backends/native_cpp_library.html" class="btn btn-neutral float-left" title="Native C++ Library" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Inference Team, STCA, Microsoft.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>